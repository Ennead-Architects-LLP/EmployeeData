#!/usr/bin/env python3
"""
Test script for the unified scraper
Tests both SIMPLE and COMPLETE modes
"""

import sys
import asyncio
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent / 'src'))

from src.core.unified_scraper import UnifiedEmployeeScraper
from src.config.settings import ScraperConfig


async def test_unified_scraper():
    """Test the unified scraper with comprehensive features"""
    print("üß™ Testing Unified Scraper")
    print("=" * 50)
    
    config = ScraperConfig.from_env()
    
    # Test unified scraper (comprehensive by default)
    print("\n1Ô∏è‚É£ Testing Unified Scraper (Comprehensive Data Extraction)")
    print("-" * 50)
    
    try:
        async with UnifiedEmployeeScraper(
            download_images=True,
            headless=True,
            timeout=10000,
            config=config
        ) as scraper:
            print(f"   Info: {scraper.get_scraper_info()}")
            print("   ‚úÖ Unified scraper initialized successfully")
            print("   üìä Features: Comprehensive data extraction enabled")
            print("   üñºÔ∏è  Images: Enabled for profile photos")
            print("   üîç Data: Name, email, phone, position, department, bio,")
            print("           office location, years with firm, seat assignment,")
            print("           computer info, memberships, education, licenses,")
            print("           projects, recent posts, social links")
            
            # Note: We're not actually running the scraper in this test
            print("   ‚ÑπÔ∏è  Skipping actual scraping (requires authentication)")
            
    except Exception as e:
        print(f"   ‚ùå Unified scraper failed: {e}")
    
    print("\n" + "=" * 50)
    print("‚úÖ Unified scraper test completed!")
    print("\nüìã Usage Examples:")
    print("   # For GitHub Actions (weekly scraper):")
    print("   scraper = UnifiedEmployeeScraper()")
    print("   ")
    print("   # For Development (main.py):")
    print("   scraper = UnifiedEmployeeScraper()")
    print("   ")
    print("   # Both use the same comprehensive interface:")
    print("   employees = await scraper.scrape_all_employees()")
    print("   ")
    print("   # All employees will have comprehensive data:")
    print("   # - Basic info (name, email, phone, position, department, bio)")
    print("   # - Advanced info (years with firm, seat, computer, memberships)")
    print("   # - Professional info (education, licenses, projects, posts)")
    print("   # - Contact info (teams, linkedin, website)")


if __name__ == "__main__":
    asyncio.run(test_unified_scraper())
