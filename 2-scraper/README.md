# Weekly Employee Data Scraper

This component handles weekly data collection from the Ennead employee directory website.

## ğŸ¯ Purpose

- **Weekly Data Collection**: Scrapes employee data from EI website
- **Image Download**: Downloads and updates profile images
- **Data Merging**: Preserves existing computer data while updating employee info
- **JSON Updates**: Saves data directly to website assets folder

## ğŸš€ Features

- âœ… **No HTML Generation** - Focuses only on data collection
- âœ… **Preserves Computer Data** - Keeps existing computer info intact
- âœ… **Image Management** - Downloads and updates profile images
- âœ… **Incremental Updates** - Only updates changed data
- âœ… **Automated Scheduling** - Runs weekly on Tuesday at 3:14 AM EST via GitHub Actions

## ğŸ“ Structure

```
2-scraper/
â”œâ”€â”€ weekly_scraper.py         # Main weekly scraper script
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ debug/                    # Debug output directory
â”‚   â”œâ”€â”€ dom_captures/         # DOM capture files
â”‚   â”œâ”€â”€ screenshots/          # Debug screenshots
â”‚   â””â”€â”€ seating_chart/        # Seating chart debug files
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ data_orchestrator.py      # Data-only orchestrator
â”‚   â”‚   â”œâ”€â”€ individual_data_orchestrator.py  # Individual employee processing
â”‚   â”‚   â”œâ”€â”€ complete_scraper.py       # Complete scraper implementation
â”‚   â”‚   â”œâ”€â”€ simple_scraper.py         # Simple scraper implementation
â”‚   â”‚   â””â”€â”€ models.py                 # Data models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ dom_capture.py            # DOM capture and analysis
â”‚   â”‚   â”œâ”€â”€ advanced_selector_recorder.py  # Advanced selector recording
â”‚   â”‚   â”œâ”€â”€ debug_utilities.py        # Debug utilities
â”‚   â”‚   â”œâ”€â”€ image_downloader.py       # Image download service
â”‚   â”‚   â”œâ”€â”€ auth.py                   # Authentication service
â”‚   â”‚   â””â”€â”€ html_generator.py         # HTML generation service
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ test_scraper.py           # Test script
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ settings.py               # Configuration
â”‚       â””â”€â”€ credentials.py            # Credentials management
â””â”€â”€ README.md                # This file
```

## ğŸ”§ Usage

### Manual Run
```bash
cd 2-scraper
pip install -r requirements.txt
python weekly_scraper.py
```

### Test Run
```bash
cd 2-scraper
python -m src.tests.test_scraper
```

### DOM Capture and Debugging
```bash
cd 2-scraper
python -m src.services.dom_capture
python -m src.services.advanced_selector_recorder
```

### GitHub Actions
The scraper runs automatically weekly on Tuesday at 3:14 AM EST (8:14 AM UTC) via GitHub Actions.

## ğŸ“Š Data Flow

1. **Load Existing Data** - Reads current individual employee JSON files
2. **Scrape New Data** - Collects employee info from EI website
3. **Merge Data** - Updates existing records, preserves computer info
4. **Download Images** - Updates profile images
5. **Save Data** - Writes to individual files in `1-website/assets/individual_employees/`

## ğŸ”„ Data Merging Logic

The scraper intelligently merges data:

- **New Employees**: Added to the dataset
- **Existing Employees**: Updated with new info
- **Computer Data**: Preserved from existing records
- **Images**: Downloaded and updated

## âš™ï¸ Configuration

### Environment Variables
- `HEADLESS`: Run browser in headless mode (default: true)
- `DOWNLOAD_IMAGES`: Download profile images (default: true)
- `TIMEOUT`: Page load timeout in milliseconds (default: 15000)

### Output
- **Employee Data**: Individual JSON files in `1-website/assets/individual_employees/`
- **Images**: `1-website/assets/images/`
- **Logs**: `weekly_scraper.log`

## ğŸ§ª Testing

Run the test script to verify functionality:

```bash
python test_scraper.py
```

This will:
- Test data loading
- Test employee scraping
- Test data merging
- Test data saving

## ğŸ“ Logging

The scraper creates detailed logs:
- **Console Output**: Real-time progress
- **Log File**: `weekly_scraper.log`
- **GitHub Actions**: Workflow logs

## ğŸ” Troubleshooting

### Common Issues

1. **No employees scraped**
   - Check network connection
   - Verify website accessibility
   - Check timeout settings

2. **Image download failures**
   - Verify image URLs
   - Check disk space
   - Review permissions

3. **Data merge issues**
   - Check JSON format
   - Verify email fields
   - Review log files

### Debug Mode

Enable debug logging:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“ˆ Performance

- **Typical Runtime**: 5-10 minutes
- **Memory Usage**: ~200MB
- **Disk Usage**: ~50MB for images
- **Network**: Downloads ~100-200 images

## ğŸ”’ Security

- **Credentials**: Stored securely in GitHub Secrets
- **Data**: No sensitive information logged
- **Access**: Read-only access to employee directory
