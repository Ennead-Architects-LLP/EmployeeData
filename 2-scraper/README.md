# Daily Employee Data Scraper

This component handles daily data collection from the Ennead employee directory website.

## ğŸ¯ Purpose

- **Daily Data Collection**: Scrapes employee data from EI website
- **Image Download**: Downloads and updates profile images
- **Data Merging**: Preserves existing computer data while updating employee info
- **JSON Updates**: Saves data directly to website assets folder

## ğŸš€ Features

- âœ… **No HTML Generation** - Focuses only on data collection
- âœ… **Preserves Computer Data** - Keeps existing computer info intact
- âœ… **Image Management** - Downloads and updates profile images
- âœ… **Incremental Updates** - Only updates changed data
- âœ… **Automated Scheduling** - Runs daily via GitHub Actions

## ğŸ“ Structure

```
2-scraper/
â”œâ”€â”€ daily_scraper.py          # Main daily scraper script
â”œâ”€â”€ test_scraper.py           # Test script
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ data_orchestrator.py  # Data-only orchestrator
â”‚   â”‚   â”œâ”€â”€ scraper.py            # Employee scraper
â”‚   â”‚   â””â”€â”€ models.py             # Data models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ image_downloader.py   # Image download service
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ settings.py           # Configuration
â””â”€â”€ README.md                # This file
```

## ğŸ”§ Usage

### Manual Run
```bash
cd 2-scraper
pip install -r requirements.txt
python daily_scraper.py
```

### Test Run
```bash
cd 2-scraper
python test_scraper.py
```

### GitHub Actions
The scraper runs automatically daily at midnight UTC via GitHub Actions.

## ğŸ“Š Data Flow

1. **Load Existing Data** - Reads current `employees_data.json`
2. **Scrape New Data** - Collects employee info from EI website
3. **Merge Data** - Updates existing records, preserves computer info
4. **Download Images** - Updates profile images
5. **Save Data** - Writes to `1-website/assets/employees_data.json`

## ğŸ”„ Data Merging Logic

The scraper intelligently merges data:

- **New Employees**: Added to the dataset
- **Existing Employees**: Updated with new info
- **Computer Data**: Preserved from existing records
- **Images**: Downloaded and updated

## âš™ï¸ Configuration

### Environment Variables
- `HEADLESS`: Run browser in headless mode (default: true)
- `DOWNLOAD_IMAGES`: Download profile images (default: true)
- `TIMEOUT`: Page load timeout in milliseconds (default: 15000)

### Output
- **Employee Data**: `1-website/assets/employees_data.json`
- **Images**: `1-website/assets/images/`
- **Logs**: `daily_scraper.log`

## ğŸ§ª Testing

Run the test script to verify functionality:

```bash
python test_scraper.py
```

This will:
- Test data loading
- Test employee scraping
- Test data merging
- Test data saving

## ğŸ“ Logging

The scraper creates detailed logs:
- **Console Output**: Real-time progress
- **Log File**: `daily_scraper.log`
- **GitHub Actions**: Workflow logs

## ğŸ” Troubleshooting

### Common Issues

1. **No employees scraped**
   - Check network connection
   - Verify website accessibility
   - Check timeout settings

2. **Image download failures**
   - Verify image URLs
   - Check disk space
   - Review permissions

3. **Data merge issues**
   - Check JSON format
   - Verify email fields
   - Review log files

### Debug Mode

Enable debug logging:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“ˆ Performance

- **Typical Runtime**: 5-10 minutes
- **Memory Usage**: ~200MB
- **Disk Usage**: ~50MB for images
- **Network**: Downloads ~100-200 images

## ğŸ”’ Security

- **Credentials**: Stored securely in GitHub Secrets
- **Data**: No sensitive information logged
- **Access**: Read-only access to employee directory
