name: Weekly Employee Data Scraper

on:
  # Triggers: run on a weekly schedule and allow manual runs from the UI
  schedule:
    # Run weekly on Tuesday at 3:14 AM EST (8:14 AM UTC)
    - cron: '14 8 * * 2' # Tuesday at 3:14 AM EST
  workflow_dispatch: # Allow manual triggering

# This workflow ensures both images and JSON artifacts are consistently generated
# and stored in docs/assets/ for GitHub Pages consumption

jobs:
  scrape-employee-data:
    runs-on: ubuntu-latest
    
    steps:
    # 1) Pull the latest repository contents so the job works on current code/data
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0  # Fetch full history for proper git operations
      
    # 2) Install a specific Python version used by the scraper
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    # 3) Install OS libraries required by Chromium/Playwright in headless CI
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libnss3 \
          libatk-bridge2.0-0 \
          libdrm2 \
          libxcomposite1 \
          libxdamage1 \
          libxrandr2 \
          libgbm1 \
          libxss1 \
          libasound2t64
          
    # 4) Install Python packages needed by the scraper
    - name: Install Python dependencies
      run: |
        cd 2-scraper
        pip install -r requirements_scraper.txt
        
    # 5) Install the Playwright browser and its dependencies for scraping
    - name: Install Playwright browsers
      run: |
        cd 2-scraper
        playwright install chromium
        playwright install-deps chromium
        
    # 6) Run the weekly scraper with credentials from repo secrets
    - name: Run weekly scraper
      env:
        SCRAPER_EMAIL: ${{ secrets.SCRAPER_EMAIL }}
        SCRAPER_PASSWORD: ${{ secrets.SCRAPER_PASSWORD }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        cd 2-scraper
        echo "Current directory: $(pwd)"
        echo "Project root should be: $(pwd)/.."
        echo "Docs directory should be: $(pwd)/../docs"

        # Run the actual scraper with headless mode and proper environment
        python -m src.main --headless
        
    # 7) Verify both images and JSON artifacts are properly generated
    - name: Verify artifacts generation
      run: |
        echo "=== Verifying JSON artifacts ==="
        echo "Individual employees (daily merge handles employees.json)"
        echo "Individual employee files count:"
        ls docs/assets/individual_employees/*.json 2>/dev/null | wc -l || echo "❌ No individual employee files found"
        echo ""
        echo "=== Verifying image artifacts ==="
        echo "Images directory:"
        ls -la docs/assets/images/ 2>/dev/null | head -10 || echo "❌ Images directory not found"
        echo "Total images count:"
        ls docs/assets/images/*.jpg 2>/dev/null | wc -l || echo "❌ No images found"
        echo ""
        echo "=== Artifact Summary ==="
        echo "JSON files: $(ls docs/assets/individual_employees/*.json 2>/dev/null | wc -l) individual employee files"
        echo "Images: $(ls docs/assets/images/*.jpg 2>/dev/null | wc -l) profile images"
        echo ""
        echo "=== Path Verification ==="
        echo "Current working directory: $(pwd)"
        echo "Docs directory exists: $(test -d docs && echo 'YES' || echo 'NO')"
        echo "Assets directory exists: $(test -d docs/assets && echo 'YES' || echo 'NO')"
        echo "Individual employees directory exists: $(test -d docs/assets/individual_employees && echo 'YES' || echo 'NO')"
        echo "Images directory exists: $(test -d docs/assets/images && echo 'YES' || echo 'NO')"
        
    # 9) Create GitHub Actions artifacts for both images and JSON
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: scraper-artifacts-${{ github.run_number }}
        path: |
          
          docs/assets/individual_employees/
          docs/assets/images/
        retention-days: 30
        
    # 10) Commit any changes (new/updated JSONs and images) and push back to the repo
    - name: Commit and push changes
      if: success()
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Only add the specific folders we care about
        git add docs/assets/individual_employees/ 2>/dev/null || true
        git add docs/assets/images/ 2>/dev/null || true
        
        # Check if there are any changes to commit
        if ! git diff --staged --quiet; then
          # Pull latest changes before pushing to avoid conflicts
          echo "Pulling latest changes from remote..."
          git pull --rebase origin main || {
            echo "Rebase failed, trying merge instead..."
            git pull origin main
          }
          
          # Commit the changes
          git commit -m "🔄 Weekly Scraper Update: $(date) - Updated employee data, JSON artifacts, and profile images"
          
          # Push with explicit remote and branch specification
          echo "Pushing changes to remote..."
          git push origin main || {
            echo "Push failed, trying to resolve conflicts..."
            git pull --rebase origin main
            git push origin main
          }
        else
          echo "No changes to commit"
        fi