name: Weekly Employee Data Scraper

on:
  # Triggers: run on a weekly schedule and allow manual runs from the UI
  schedule:
    # Run weekly on Tuesday at 3:14 AM EST (8:14 AM UTC)
    - cron: '14 8 * * 2' # Tuesday at 3:14 AM EST
  workflow_dispatch: # Allow manual triggering

jobs:
  scrape-employee-data:
    runs-on: ubuntu-latest
    
    steps:
    # 1) Pull the latest repository contents so the job works on current code/data
    - name: Checkout repository
      uses: actions/checkout@v4
      
    # 2) Install a specific Python version used by the scraper
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    # 3) Install OS libraries required by Chromium/Playwright in headless CI
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libnss3 \
          libatk-bridge2.0-0 \
          libdrm2 \
          libxcomposite1 \
          libxdamage1 \
          libxrandr2 \
          libgbm1 \
          libxss1 \
          libasound2t64
          
    # 4) Install Python packages needed by the scraper
    - name: Install Python dependencies
      run: |
        cd 2-scraper
        pip install -r requirements-linux.txt
        
    # 5) Install the Playwright browser and its dependencies for scraping
    - name: Install Playwright browsers
      run: |
        cd 2-scraper
        playwright install chromium
        playwright install-deps chromium
        
    # 6) Run the weekly scraper with credentials from repo secrets
    - name: Run weekly scraper
      env:
        SCRAPER_EMAIL: ${{ secrets.SCRAPER_EMAIL }}
        SCRAPER_PASSWORD: ${{ secrets.SCRAPER_PASSWORD }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        cd 2-scraper
        python weekly_scraper.py
        
    # 7) Build the employee file indexes used by the website (writes to docs/)
    - name: Generate employee files list
      run: |
        echo "Running .github/scripts/generate_index_for_json.py..."
        python .github/scripts/generate_index_for_json.py
        echo "Generated list in docs/assets/:"
        ls -la docs/assets/*.json || true
        
    # 8) Copy generated lists and all individual employee JSONs to root/assets
    #    so GitHub Pages (served from the repo root) can fetch them directly
    - name: Copy list and individual files to root for GitHub Pages access
      run: |
        mkdir -p assets
        cp docs/assets/employee_files_list.json assets/
        # Copy individual employee files to root assets directory
        cp -r docs/assets/individual_employees assets/
        echo "Copied JSON files and individual employee data to root directory"
        
    # 9) Commit any changes (new/updated JSONs) and push back to the repo
    - name: Commit and push changes
      if: success()
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add .
        git add docs/assets/employee_files_list.json
        git add docs/assets/individual_employees/
        git add assets/employee_files_list.json
        git add assets/individual_employees/
        git diff --staged --quiet || git commit -m "$$$_Action_Weekly_Scraper_Update: $(date) - Updated employee data and employee_files_list.json"
        git push